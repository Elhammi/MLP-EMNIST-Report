# -*- coding: utf-8 -*-
"""MLP_PyTorch.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1lmimJ0eL0S_d9QoUJrnlF8Y1ZeFPDmw5
"""

import argparse
from copy import deepcopy
from matplotlib import pyplot as plt
import numpy as np

import torch
import torch.nn.functional as F
from torch import nn, optim
from torch.utils.data import TensorDataset
import torchvision 
import torchvision.transforms as transforms
from sklearn.model_selection import train_test_split

device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')

Edata = torchvision.datasets.EMNIST(root='emnist',split='balanced' , download=True)

print(Edata.classes)
print(str(len(Edata.classes))+ 'classes')
print('\nData size:')
print(Edata.data.shape)
images = Edata.data.view([112800,1,28,28]).float()
print('\nTensor data:')
print(images.shape)

print(torch.sum(Edata.targets == 0))
torch.unique(Edata.targets)

import copy
letterCategories = Edata.classes[1:]
labels = copy.deepcopy(Edata.targets)-1
print(labels.shape)
print(torch.sum(labels==0))
torch.unique(labels)

transformCustom = transforms.Compose([
                                      transforms.ToTensor(), #convert to tensor
                                      transforms.Lambda(lambda x:x.view(-1)) #flatten 28*28 into 784 vector for each image
                                      ])

train = torchvision.datasets.EMNIST(root='.',train=True,transform=transformCustom, download=True,split='balanced')
test = torchvision.datasets.EMNIST(root='.',train=False,transform=transformCustom, download=True,split='balanced')

print(train)
print(train.train_data.shape)
print(train.train_labels.shape)

import pandas as pd
tests = pd.Series(train.train_labels).value_counts().sort_index()

label_list = list('0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZabdefghnqrt')

print('"classes"   "Number"   "Percentage"')
total = int(train.train_labels.shape[0])
for i,letter in enumerate(label_list):
  print(f'Letter {letter} :   {tests[i]} \t({(int(tests[i])/total)*100:.2f}%)')

from torch.utils.data.dataloader import DataLoader
from torch.utils.data.dataset import TensorDataset
batchSize = 128 
train_loader = DataLoader(train, batch_size = batchSize, shuffle=True)
test_loader = DataLoader(test, batch_size = batchSize, shuffle=False)

examples = iter(train_loader)
samples, labels = next(examples)
print(samples.shape, labels.shape)

for i in range(8):
  plt.subplot(2,4,i+1)
  plt.title(label_list[labels[i].detach().numpy()])
  plt.imshow(samples[i].reshape((28,28)), cmap='gray')

count_batch = 0
for x,y in train_loader:
  count_batch+=1

print(f'Number of batches in train dataset: {count_batch} ')

count_test = 0
for x,y in test_loader:
  count_test+=1

print(f'Number of batches in train dataset: {count_test} ')

# MLP_1: 1 hidden layer
class MLP_1(nn.Module):
  def __init__(self,input_size, hidden_size, num_classes):
    super(MLP_1,self).__init__()
    self.layer1 = nn.Linear(input_size,hidden_size)
    self.layer2 = nn.Linear(hidden_size,num_classes)

  def forward(self,x):
    out = self.layer1(x)
    out = F.sigmoid(out)
    out = self.layer2(out)
    if not self.training:
      out = F.softmax(out,dim=1) 
    return out

# MLP_2: 2 hidden layer normal
class MLP_2(nn.Module):
  def __init__(self,input_size, hidden_size1, hidden_size2, num_classes):
    super(MLP_2,self).__init__()
    self.layer1 = nn.Linear(input_size,hidden_size1)
    self.layer2 = nn.Linear(hidden_size1,hidden_size2)
    self.layer3 = nn.Linear(hidden_size2,num_classes)

  def forward(self,x):
    out = self.layer1(x)
    out = F.sigmoid(out)
    out = self.layer2(out)
    out = F.sigmoid(out)
    out = self.layer3(out)
    if not self.training:
      out = F.softmax(out,dim=1)
    return out

# MLP_2_DROP: 2 hidden layer with dropout
class MLP_2_DROP(nn.Module):
  def __init__(self,input_size, hidden_size1, hidden_size2, num_classes):
    super(MLP_2_DROP,self).__init__()
    self.layer1 = nn.Linear(input_size,hidden_size1)
    self.layer2 = nn.Linear(hidden_size1,hidden_size2)
    self.layerDrop = nn.Dropout(0.2)
    self.layer3 = nn.Linear(hidden_size2,num_classes)

  def forward(self,x):
    out = self.layer1(x)
    out = F.sigmoid(out)
    out = self.layer2(out)
    out = F.sigmoid(out)
    out = self.layerDrop(out)
    out = self.layer3(out)
    if not self.training:
      out = F.softmax(out,dim=1) 
    return out

# MLP_3: 3 hidden layer
class MLP_3(nn.Module):
  def __init__(self,input_size, hidden_size1, hidden_size2, hidden_size3, num_classes):
    super(MLP_3,self).__init__()
    self.layer1 = nn.Linear(input_size,hidden_size1)
    self.layer2 = nn.Linear(hidden_size1,hidden_size2)
    self.layer3 = nn.Linear(hidden_size2,hidden_size3)
    self.layer4 = nn.Linear(hidden_size3,num_classes)

  def forward(self,x):
    out = self.layer1(x)
    out = F.sigmoid(out)
    out = self.layer2(out)
    out = F.sigmoid(out)
    out = self.layer3(out)
    out = F.sigmoid(out)
    out = self.layer4(out)
    if not self.training:
      out = F.softmax(out,dim=1) 
    return out

input_size = 784
num_classes = 47

model_layer1 = MLP_1(input_size, 570, num_classes)
model_layer2 = MLP_2(input_size, 570, 427, num_classes)
model_layer2drop = MLP_2_DROP(input_size, 570, 427, num_classes)
model_layer3 = MLP_3(input_size, 570, 427, 332, num_classes)

model_layer1.load_state_dict(torch.load('model1.weights'))
model_layer2.load_state_dict(torch.load('model2.weights'))
model_layer2drop.load_state_dict(torch.load('model2drop.weights'))
model_layer3.load_state_dict(torch.load('model3.weights'))

num_epochs = 10

loss_fn = nn.CrossEntropyLoss()
opt1 = torch.optim.Adam(model_layer1.parameters()) 
opt2n = torch.optim.Adam(model_layer2.parameters())
opt2d = torch.optim.Adam(model_layer2drop.parameters())
opt3 = torch.optim.Adam(model_layer3.parameters())

loss_epoch1 = []
loss_epoch2n = []
loss_epoch2d = []
loss_epoch3 = []

print("Training 1 hidden Layer:")
for epoch in range(num_epochs):
  model_layer1.train()
  loss = 0

  for input, target_batch in train_loader:
    opt1.zero_grad() 
    predict_batch = model_layer1(input) #forward
    loss_batch = loss_fn(predict_batch, target_batch) #loss
    loss_batch.backward() #backward

    opt1.step() #update weights
    loss += loss_batch.item()

  loss_epoch1.append(loss)
  print(f'Epoch: {epoch+1}  loss: {loss}')
print(f'Learning parameters is: {sum(p.numel() for p in model_layer1.parameters() if p.requires_grad)}')

print("*****************************************\n")
print("Training 2 hidden Layer Normal:")
for epoch in range(num_epochs):
  model_layer2.train()
  loss = 0

  for input, target_batch in train_loader:
    opt2n.zero_grad()
    predict_batch = model_layer2(input) #forward
    loss_batch = loss_fn(predict_batch, target_batch) #loss
    loss_batch.backward() #backward

    opt2n.step() #update weights
    loss += loss_batch.item()

  loss_epoch2n.append(loss)
  print(f'Epoch: {epoch+1}  loss: {loss}')
print(f'Learning parameters is: {sum(p.numel() for p in model_layer2.parameters() if p.requires_grad)}')

print("*****************************************\n")
print("Training 2 hidden Layer With Dropout:")
for epoch in range(num_epochs):
  model_layer2drop.train()
  loss = 0

  for input, target_batch in train_loader:
    opt2d.zero_grad() 
    predict_batch = model_layer2drop(input) #forward
    loss_batch = loss_fn(predict_batch, target_batch) #loss
    loss_batch.backward() #backward

    opt2d.step() #update weights
    loss += loss_batch.item()

  loss_epoch2d.append(loss)
  print(f'Epoch: {epoch+1}  loss: {loss}')
print(f'Learning parameters is: {sum(p.numel() for p in model_layer2drop.parameters() if p.requires_grad)}')

print("*****************************************\n")
print("Training 3 hidden Layer:")
for epoch in range(num_epochs):
  model_layer3.train()
  loss = 0

  for input, target_batch in train_loader:
    opt3.zero_grad() 
    predict_batch = model_layer3(input) #forward
    loss_batch = loss_fn(predict_batch, target_batch) #loss
    loss_batch.backward() #backward

    opt3.step() #update weights
    loss += loss_batch.item()

  loss_epoch3.append(loss)
  print(f'Epoch: {epoch+1}  loss: {loss}')
print(f'Learning parameters is: {sum(p.numel() for p in model_layer3.parameters() if p.requires_grad)}')

losses = [loss_epoch1,loss_epoch2n,loss_epoch2d,loss_epoch3]

with open('./losses.txt', 'a') as testwritefile:
    testwritefile.write(str(losses))

loss_epoch1,loss_epoch2n,loss_epoch2d,loss_epoch3 = [],[],[],[]
losses = [loss_epoch1,loss_epoch2n,loss_epoch2d,loss_epoch3]

with open('./losses.txt', 'r') as testwritefile:
    lossess = testwritefile.read()

for losses_each, losses_list in zip(lossess.split("[[")[1].split("]]")[0].split("], ["),losses):
  for each in losses_each.split(","):
    losses_list.append(float(each.strip()))

num_epochs = 10

plt.plot(range(num_epochs), loss_epoch1, label="1 Hidden Layers")
plt.plot(range(num_epochs), loss_epoch2n, label="2 Hidden Layers")
plt.plot(range(num_epochs), loss_epoch2d, label="2 Hidden Layers with Dropout")
plt.plot(range(num_epochs), loss_epoch3, label="3 Hidden Layers")
plt.title('Training loss')
plt.xlabel("Epoch number")
plt.ylabel("Loss")
plt.legend()

torch.save(model_layer1.state_dict(), "./model1.weights")
torch.save(model_layer2.state_dict(), "./model2.weights")
torch.save(model_layer2drop.state_dict(), "./model2drop.weights")
torch.save(model_layer3.state_dict(), "./model3.weights")

model_layer1 = model_layer1.eval()
model_layer2 = model_layer2.eval()
model_layer2drop = model_layer2drop.eval()
model_layer3 = model_layer3.eval()

def eval_model_acc_err(loader,model):
  n_correct = 0
  n_total = 0

  for input_batch, target_batch in loader:
    predict_batch = model(input_batch) 
    _, predictions = torch.max(predict_batch, 1)
    n_total += target_batch.shape[0]
    n_correct += (predictions == target_batch).sum().item()

  acc = round((n_correct / n_total)*100,2)
  err = round(1-(n_correct / n_total),2)
  # print(f'data number: {n_total}')
  # print(f'wrong predicted label nuber: {n_total - n_correct}')
  # print(f'Accuracy: {acc}')
  return acc,err

print('1 hidden Layer...')
print("Training Data:")
train1_acc,train1_err = eval_model_acc_err(train_loader, model_layer1)
print(f'training error : {train1_err}')
print(f'training accuracy {train1_acc:.2f}%')
print('-------------------------------------------\n')

print("\nTesting Data:")
test1_acc,test1_err = eval_model_acc_err(test_loader, model_layer1)
print(f'Testing error : {test1_err}')
print(f'Testing accuracy {test1_acc:.2f}%')
print('*******************************************\n')


print('2 hidden Layer Normal...')
train2n_acc,train2n_err = eval_model_acc_err(train_loader, model_layer2)
print(f'training error : {train2n_err}')
print(f'training accuracy {train2n_acc:.2f}%')
print('-------------------------------------------\n')

test2n_acc,test2n_err = eval_model_acc_err(test_loader, model_layer2)
print(f'Testing error : {test2n_err}')
print(f'Testing accuracy {test2n_acc:.2f}%')
print('*******************************************\n')


print('2 hidden Layer with Dropout...')
train2d_acc,train2d_err = eval_model_acc_err(train_loader, model_layer2drop)
print(f'training error : {train2d_err}')
print(f'training accuracy {train2d_acc:.2f}%')
print('-------------------------------------------\n')

test2d_acc,test2d_err = eval_model_acc_err(test_loader, model_layer2drop)
print(f'Testing error : {test2d_err}')
print(f'Testing accuracy {test2d_acc:.2f}%')
print('*******************************************\n')


print('3 hidden Layer...')
train3_acc,train3_err = eval_model_acc_err(train_loader, model_layer3)
print(f'training error : {train3_err}')
print(f'training accuracy {train3_acc:.2f}%')
print('-------------------------------------------\n')

test3_acc,test3_err = eval_model_acc_err(test_loader, model_layer3)
print(f'Testing error : {test3_err}')
print(f'Testing accuracy {test3_acc:.2f}%')

params_m1 = sum(p.numel() for p in model_layer1.parameters() if p.requires_grad)
params_m2 = sum(p.numel() for p in model_layer2.parameters() if p.requires_grad)
params_m2drop = sum(p.numel() for p in model_layer2drop.parameters() if p.requires_grad)
params_m3 = sum(p.numel() for p in model_layer3.parameters() if p.requires_grad)
params_all = [params_m1,params_m2,params_m2drop,params_m3]

train_acc_all = [train1_acc,train2n_acc,train2d_acc,train3_acc]
test_acc_all = [test1_acc,test2n_acc,test2d_acc,test3_acc]

train_err_all = [train1_err,train2n_err,train2d_err,train3_err]
test_err_all = [test1_err,test2n_err,test2d_err,test3_err]

plt.plot(params_all,train_acc_all,marker='^',label='Train data')
plt.plot(params_all,test_acc_all,marker='^',label='Test data')
plt.title('Training and Testing Accuracy')
plt.xlabel("Model Parameters")
plt.ylabel("Accuracy %")
plt.legend()